#!/usr/bin/env python3
"""
H·ªá th·ªëng ph√°t hi·ªán l·ª≠a s·ª≠ d·ª•ng Vector Database
So s√°nh ƒë·∫∑c tr∆∞ng c·ªßa ·∫£nh m·ªõi v·ªõi database ƒë√£ l∆∞u
"""

import cv2
import numpy as np
import os
import pickle
import joblib
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
from datetime import datetime
import json
from typing import Dict, List, Tuple, Any
import seaborn as sns

class FireFeatureExtractor:
    """Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ ·∫£nh"""
    
    def __init__(self):
        # ƒê·ªãnh nghƒ©a c√°c ng∆∞·ª°ng m√†u l·ª≠a
        self.fire_color_ranges = {
            'red_lower': np.array([0, 100, 100]),
            'red_upper': np.array([10, 255, 255]),
            'orange_lower': np.array([10, 100, 100]),
            'orange_upper': np.array([25, 255, 255]),
            'yellow_lower': np.array([25, 100, 100]),
            'yellow_upper': np.array([35, 255, 255])
        }
    
    def preprocess_image(self, image_path: str) -> Dict[str, np.ndarray]:
        """Ti·ªÅn x·ª≠ l√Ω ·∫£nh"""
        # Load ·∫£nh
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"Kh√¥ng th·ªÉ load ·∫£nh: {image_path}")
        
        # Resize v·ªÅ k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh
        image = cv2.resize(image, (224, 224))
        
        # Chuy·ªÉn ƒë·ªïi m√†u
        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        return {
            'original': image,
            'rgb': rgb,
            'hsv': hsv,
            'gray': gray
        }
    
    def extract_color_histogram(self, hsv_image: np.ndarray) -> np.ndarray:
        """Tr√≠ch xu·∫•t histogram m√†u s·∫Øc"""
        # Histogram cho t·ª´ng k√™nh m√†u
        h_hist = cv2.calcHist([hsv_image], [0], None, [180], [0, 180])
        s_hist = cv2.calcHist([hsv_image], [1], None, [256], [0, 256])
        v_hist = cv2.calcHist([hsv_image], [2], None, [256], [0, 256])
        
        # Chu·∫©n h√≥a
        h_hist = cv2.normalize(h_hist, h_hist).flatten()
        s_hist = cv2.normalize(s_hist, s_hist).flatten()
        v_hist = cv2.normalize(v_hist, v_hist).flatten()
        
        # K·∫øt h·ª£p th√†nh m·ªôt vector
        color_hist = np.concatenate([h_hist, s_hist, v_hist])
        return color_hist
    
    def extract_fire_color_mask(self, hsv_image: np.ndarray) -> Tuple[Dict[str, float], np.ndarray]:
        """Tr√≠ch xu·∫•t mask m√†u l·ª≠a"""
        # T·∫°o mask cho t·ª´ng m√†u
        red_mask = cv2.inRange(hsv_image, self.fire_color_ranges['red_lower'], self.fire_color_ranges['red_upper'])
        orange_mask = cv2.inRange(hsv_image, self.fire_color_ranges['orange_lower'], self.fire_color_ranges['orange_upper'])
        yellow_mask = cv2.inRange(hsv_image, self.fire_color_ranges['yellow_lower'], self.fire_color_ranges['yellow_upper'])
        
        # K·∫øt h·ª£p mask
        fire_mask = cv2.bitwise_or(red_mask, orange_mask)
        fire_mask = cv2.bitwise_or(fire_mask, yellow_mask)
        
        # T√≠nh t·ª∑ l·ªá
        total_pixels = hsv_image.shape[0] * hsv_image.shape[1]
        red_ratio = np.sum(red_mask > 0) / total_pixels
        orange_ratio = np.sum(orange_mask > 0) / total_pixels
        yellow_ratio = np.sum(yellow_mask > 0) / total_pixels
        total_fire_ratio = np.sum(fire_mask > 0) / total_pixels
        
        fire_features = {
            'red_ratio': red_ratio,
            'orange_ratio': orange_ratio,
            'yellow_ratio': yellow_ratio,
            'total_fire_ratio': total_fire_ratio,
            'fire_pixels': np.sum(fire_mask > 0)
        }
        
        return fire_features, fire_mask
    
    def extract_texture_features(self, gray_image: np.ndarray) -> Dict[str, float]:
        """Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng texture"""
        # Gradient
        grad_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)
        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
        
        # Th·ªëng k√™ gradient
        gradient_mean = np.mean(gradient_magnitude)
        gradient_std = np.std(gradient_magnitude)
        
        # Entropy
        hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])
        hist = hist / np.sum(hist)
        entropy = -np.sum(hist * np.log2(hist + 1e-10))
        
        # Local Binary Pattern (ƒë∆°n gi·∫£n)
        lbp = self._compute_lbp(gray_image)
        lbp_hist = cv2.calcHist([lbp], [0], None, [256], [0, 256])
        lbp_hist = cv2.normalize(lbp_hist, lbp_hist).flatten()
        
        texture_features = {
            'gradient_mean': gradient_mean,
            'gradient_std': gradient_std,
            'entropy': entropy,
            'lbp_histogram': lbp_hist
        }
        
        return texture_features
    
    def _compute_lbp(self, image: np.ndarray) -> np.ndarray:
        """T√≠nh Local Binary Pattern ƒë∆°n gi·∫£n"""
        lbp = np.zeros_like(image)
        for i in range(1, image.shape[0]-1):
            for j in range(1, image.shape[1]-1):
                center = image[i, j]
                code = 0
                # 8 neighbors
                neighbors = [
                    image[i-1, j-1], image[i-1, j], image[i-1, j+1],
                    image[i, j+1], image[i+1, j+1], image[i+1, j],
                    image[i+1, j-1], image[i, j-1]
                ]
                for k, neighbor in enumerate(neighbors):
                    if neighbor >= center:
                        code += 2**k
                lbp[i, j] = code
        return lbp.astype(np.uint8)
    
    def extract_all_features(self, image_path: str) -> Dict[str, Any]:
        """Tr√≠ch xu·∫•t t·∫•t c·∫£ ƒë·∫∑c tr∆∞ng"""
        # Ti·ªÅn x·ª≠ l√Ω
        processed = self.preprocess_image(image_path)
        
        # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng
        color_hist = self.extract_color_histogram(processed['hsv'])
        fire_features, fire_mask = self.extract_fire_color_mask(processed['hsv'])
        texture_features = self.extract_texture_features(processed['gray'])
        
        # T·∫°o vector ƒë·∫∑c tr∆∞ng
        feature_vector = {
            'color_histogram': color_hist,
            'fire_features': fire_features,
            'texture_features': texture_features,
            'processed_images': processed,
            'fire_mask': fire_mask
        }
        
        return feature_vector

class VectorBasedFireClassifier:
    """Classifier d·ª±a tr√™n vector database"""
    
    def __init__(self, vector_db_path: str = "vector_database.pkl"):
        self.vector_db_path = vector_db_path
        self.vector_database = None
        self.scaler = StandardScaler()
        self.classifier = RandomForestClassifier(n_estimators=100, random_state=42)
        self.feature_extractor = FireFeatureExtractor()
        
    def create_feature_vector(self, feature_data: Dict[str, Any]) -> np.ndarray:
        """T·∫°o vector ƒë·∫∑c tr∆∞ng t·ª´ d·ªØ li·ªáu ƒë√£ tr√≠ch xu·∫•t"""
        vectors = []
        
        # 1. Color histogram (692 features: 180 + 256 + 256)
        vectors.append(feature_data['color_histogram'])
        
        # 2. Fire features (5 features)
        fire_feat = list(feature_data['fire_features'].values())
        vectors.append(fire_feat)
        
        # 3. Texture features (3 + 256 features)
        texture_feat = [
            feature_data['texture_features']['gradient_mean'],
            feature_data['texture_features']['gradient_std'],
            feature_data['texture_features']['entropy']
        ]
        vectors.append(texture_feat)
        vectors.append(feature_data['texture_features']['lbp_histogram'])
        
        # K·∫øt h·ª£p t·∫•t c·∫£ th√†nh m·ªôt vector
        combined_vector = np.concatenate(vectors)
        return combined_vector
    
    def build_vector_database(self, dataset_path: str, labels_path: str = None):
        """X√¢y d·ª±ng vector database t·ª´ dataset"""
        print("üî® ƒêang x√¢y d·ª±ng vector database...")
        
        # T√¨m t·∫•t c·∫£ ·∫£nh trong dataset
        image_files = []
        labels = []
        
        # T√¨m ·∫£nh trong c√°c th∆∞ m·ª•c con
        for root, dirs, files in os.walk(dataset_path):
            for file in files:
                if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                    image_path = os.path.join(root, file)
                    image_files.append(image_path)
                    
                    # X√°c ƒë·ªãnh label t·ª´ t√™n file ho·∫∑c th∆∞ m·ª•c
                    if 'fire' in file.lower() or 'train_' in file.lower():
                        labels.append(1)  # C√≥ l·ª≠a
                    else:
                        labels.append(0)  # Kh√¥ng c√≥ l·ª≠a
        
        print(f"üìä T√¨m th·∫•y {len(image_files)} ·∫£nh")
        
        # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng
        all_vectors = []
        all_labels = []
        image_paths = []
        
        for i, (image_path, label) in enumerate(zip(image_files, labels)):
            try:
                print(f"üîç ƒêang x·ª≠ l√Ω {i+1}/{len(image_files)}: {os.path.basename(image_path)}")
                
                # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng
                features = self.feature_extractor.extract_all_features(image_path)
                vector = self.create_feature_vector(features)
                
                all_vectors.append(vector)
                all_labels.append(label)
                image_paths.append(image_path)
                
            except Exception as e:
                print(f"‚ùå L·ªói khi x·ª≠ l√Ω {image_path}: {e}")
                continue
        
        # Chu·∫©n h√≥a vectors
        all_vectors = np.array(all_vectors)
        scaled_vectors = self.scaler.fit_transform(all_vectors)
        
        # Hu·∫•n luy·ªán classifier
        print("üéØ ƒêang hu·∫•n luy·ªán classifier...")
        self.classifier.fit(scaled_vectors, all_labels)
        
        # ƒê√°nh gi√°
        predictions = self.classifier.predict(scaled_vectors)
        accuracy = accuracy_score(all_labels, predictions)
        print(f"‚úÖ ƒê·ªô ch√≠nh x√°c: {accuracy:.3f}")
        
        # L∆∞u vector database
        vector_database = {
            'all_vectors': scaled_vectors,
            'all_labels': all_labels,
            'image_paths': image_paths,
            'feature_dimension': scaled_vectors.shape[1],
            'total_samples': len(all_labels)
        }
        
        with open(self.vector_db_path, 'wb') as f:
            pickle.dump(vector_database, f)
        
        # L∆∞u scaler v√† classifier
        joblib.dump(self.scaler, 'scaler.pkl')
        joblib.dump(self.classifier, 'classifier.pkl')
        
        self.vector_database = vector_database
        print(f"üíæ Vector database ƒë√£ ƒë∆∞·ª£c l∆∞u: {self.vector_db_path}")
        print(f"üìä K√≠ch th∆∞·ªõc database: {len(all_vectors)} vectors")
        print(f"üî¢ Chi·ªÅu ƒë·∫∑c tr∆∞ng: {scaled_vectors.shape[1]}")
        
        return vector_database
    
    def load_vector_database(self):
        """Load vector database t·ª´ file"""
        if os.path.exists(self.vector_db_path):
            with open(self.vector_db_path, 'rb') as f:
                self.vector_database = pickle.load(f)
            
            # Load scaler v√† classifier
            if os.path.exists('scaler.pkl'):
                self.scaler = joblib.load('scaler.pkl')
            if os.path.exists('classifier.pkl'):
                self.classifier = joblib.load('classifier.pkl')
            
            print(f"‚úÖ Vector database ƒë√£ ƒë∆∞·ª£c load")
            print(f"üìä K√≠ch th∆∞·ªõc: {self.vector_database['total_samples']} vectors")
            print(f"üî¢ Chi·ªÅu ƒë·∫∑c tr∆∞ng: {self.vector_database['feature_dimension']}")
            return True
        else:
            print("‚ùå Kh√¥ng t√¨m th·∫•y vector database")
            return False
    
    def classify_new_image(self, image_path: str) -> Dict[str, Any]:
        """Ph√¢n lo·∫°i ·∫£nh m·ªõi"""
        if self.vector_database is None:
            raise ValueError("Vector database ch∆∞a ƒë∆∞·ª£c load")
        
        # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng
        features = self.feature_extractor.extract_all_features(image_path)
        vector = self.create_feature_vector(features)
        
        # Chu·∫©n h√≥a
        scaled_vector = self.scaler.transform([vector])[0]
        
        # D·ª± ƒëo√°n
        prediction = self.classifier.predict([scaled_vector])[0]
        probability = self.classifier.predict_proba([scaled_vector])[0]
        
        # T√¨m ·∫£nh t∆∞∆°ng t·ª±
        similarities = self._calculate_similarities(scaled_vector)
        
        result = {
            'image_path': image_path,
            'prediction': 'FIRE' if prediction == 1 else 'NO FIRE',
            'confidence': max(probability),
            'probability_fire': probability[1],
            'probability_no_fire': probability[0],
            'similar_images': similarities,
            'features': {
                'fire_ratio': features['fire_features']['total_fire_ratio'],
                'red_ratio': features['fire_features']['red_ratio'],
                'orange_ratio': features['fire_features']['orange_ratio'],
                'yellow_ratio': features['fire_features']['yellow_ratio'],
                'texture_entropy': features['texture_features']['entropy']
            }
        }
        
        return result
    
    def _calculate_similarities(self, query_vector: np.ndarray) -> List[Dict]:
        """T√≠nh ƒë·ªô t∆∞∆°ng t·ª± v·ªõi c√°c ·∫£nh trong database"""
        similarities = []
        
        for i, stored_vector in enumerate(self.vector_database['all_vectors']):
            # Cosine similarity
            similarity = np.dot(query_vector, stored_vector) / (
                np.linalg.norm(query_vector) * np.linalg.norm(stored_vector)
            )
            
            similarities.append({
                'index': i,
                'similarity': similarity,
                'label': self.vector_database['all_labels'][i],
                'image_path': self.vector_database['image_paths'][i]
            })
        
        # S·∫Øp x·∫øp theo ƒë·ªô t∆∞∆°ng t·ª± gi·∫£m d·∫ßn
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        return similarities[:5]  # Tr·∫£ v·ªÅ top 5
    
    def visualize_similarities(self, image_path: str, save_path: str = None):
        """Visualize ·∫£nh g·ªëc v√† c√°c ·∫£nh t∆∞∆°ng t·ª±"""
        result = self.classify_new_image(image_path)
        
        # Load ·∫£nh g·ªëc
        original = cv2.imread(image_path)
        original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)
        
        # T·∫°o subplot
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        # ·∫¢nh g·ªëc
        axes[0, 0].imshow(original)
        axes[0, 0].set_title(f"·∫¢nh g·ªëc\n{result['prediction']} ({result['confidence']:.2f})")
        axes[0, 0].axis('off')
        
        # 5 ·∫£nh t∆∞∆°ng t·ª± nh·∫•t
        for i, similar in enumerate(result['similar_images'][:5]):
            row = (i + 1) // 3
            col = (i + 1) % 3
            
            if row < 2:
                similar_img = cv2.imread(similar['image_path'])
                similar_img = cv2.cvtColor(similar_img, cv2.COLOR_BGR2RGB)
                similar_img = cv2.resize(similar_img, (224, 224))
                
                label = "FIRE" if similar['label'] == 1 else "NO FIRE"
                axes[row, col].imshow(similar_img)
                axes[row, col].set_title(f"Similar {i+1}\n{label} ({similar['similarity']:.3f})")
                axes[row, col].axis('off')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"üñºÔ∏è Visualization ƒë√£ ƒë∆∞·ª£c l∆∞u: {save_path}")
        
        plt.show()
        return result

def main():
    """H√†m ch√≠nh ƒë·ªÉ test h·ªá th·ªëng"""
    classifier = VectorBasedFireClassifier()
    
    # Ki·ªÉm tra xem c√≥ vector database kh√¥ng
    if not classifier.load_vector_database():
        print("üî® C·∫ßn x√¢y d·ª±ng vector database tr∆∞·ªõc...")
        print("üìÅ ƒêang t√¨m dataset...")
        
        # T√¨m dataset
        dataset_path = "../dataset"
        if os.path.exists(dataset_path):
            classifier.build_vector_database(dataset_path)
        else:
            print("‚ùå Kh√¥ng t√¨m th·∫•y dataset")
            return
    
    # Test v·ªõi ·∫£nh m·ªõi
    test_images = [
        "../dataset/train/images/train_1.jpg",
        "../dataset/train/images/train_7.jpg",
        "../dataset/train/images/train_100.jpg"
    ]
    
    for test_image in test_images:
        if os.path.exists(test_image):
            print(f"\nüîç Test v·ªõi: {test_image}")
            result = classifier.classify_new_image(test_image)
            
            print(f"üéØ K·∫øt qu·∫£: {result['prediction']}")
            print(f"üìä ƒê·ªô tin c·∫≠y: {result['confidence']:.3f}")
            print(f"üî• X√°c su·∫•t c√≥ l·ª≠a: {result['probability_fire']:.3f}")
            print(f"‚ùå X√°c su·∫•t kh√¥ng l·ª≠a: {result['probability_no_fire']:.3f}")
            print(f"üìà T·ª∑ l·ªá m√†u l·ª≠a: {result['features']['fire_ratio']:.3f}")
            
            # Hi·ªÉn th·ªã ·∫£nh t∆∞∆°ng t·ª± nh·∫•t
            top_similar = result['similar_images'][0]
            print(f"üñºÔ∏è ·∫¢nh t∆∞∆°ng t·ª± nh·∫•t: {os.path.basename(top_similar['image_path'])} (similarity: {top_similar['similarity']:.3f})")

if __name__ == "__main__":
    main() 